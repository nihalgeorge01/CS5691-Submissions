{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Logistic Regression\n",
    "# \n",
    "# Load features from file, then apply model\n",
    "# Try diff params, give accuracies, misclassification rates for each dataset\n",
    "# Plot confusion matrix, ROC, DET\n",
    "# Do not use inbuilt library for Logistic Regression\n",
    "# Do on three forms of data -- normal, after PCA, after LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take command line args\n",
    "# feature type - normal, LDA, PCA\n",
    "# dataset - Image, Synthetic, IsolatedDigits, HandwrittenCharacters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "INF = 9999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(v):\n",
    "    return np.exp(-v)/np.sum(np.exp(-v), axis=1)\n",
    "\n",
    "def label2onehot(y_l):\n",
    "    classes = sorted(list(np.unique(y_l)))\n",
    "    cl_ct = len(classes)\n",
    "    M = y_l.shape[0]\n",
    "\n",
    "    cl2ind = {classes[i]:i for i in range(cl_ct)}\n",
    "\n",
    "    y_o = np.zeros([M, cl_ct])\n",
    "    for i in range(M):\n",
    "        y_o[i, cl2ind[y_l[i]]] = 1\n",
    "    \n",
    "    return y_o\n",
    "\n",
    "class MulticlassLR():\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X_train, y_train, iter_ct = 1000, lr = 0.1, reg = 0.001):\n",
    "        # Make one-hot encoding for classes (for OvA crossentropy)\n",
    "        y_train_o = label2onehot(y_train)\n",
    "        cl_ct = y_train_o.shape[1]\n",
    "        M = X_train.shape[0]\n",
    "\n",
    "        # Define hypothesis Wx. Zero initialisation\n",
    "        W = np.zeros([X_train.shape[1], cl_ct])\n",
    "        Z = - X_train @ W\n",
    "        Z_s = softmax(Z)\n",
    "        loss_here = (1/M) * (np.trace(X_train @ W @ y_train_o.T) + np.sum(np.log(np.sum(np.exp(Z), axis=1))))\n",
    "        losses = [loss_here]\n",
    "        # Gradient descent iterations\n",
    "        for i in range(iter_ct):\n",
    "            grad = (1/M) * (X_train.T @ (y_train_o-Z_s)) + 2*reg*W # L2 reg\n",
    "            W -= lr*grad\n",
    "            loss_here = (1/M) * (np.trace(X_train @ W @ y_train_o.T) + np.sum(np.log(np.sum(np.exp(Z), axis=1))))\n",
    "            losses.append(loss_here)\n",
    "            Z = - X_train @ W\n",
    "            Z_s = softmax(Z)\n",
    "        \n",
    "        self.W = W\n",
    "\n",
    "    def predict(self, X_dev):\n",
    "        Z = - X_dev @ self.W\n",
    "        Z_s = softmax(Z)\n",
    "        y_pred = np.argmax(Z_s, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "    def get_acc(self, X_dev, y_dev):\n",
    "        y_pred = self.predict(X_dev)\n",
    "        correct = np.sum(y_pred == y_dev)\n",
    "        tot = y_pred.shape[0]\n",
    "        acc = correct/tot\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# def loss(X, Y, W):\n",
    "#     \"\"\"\n",
    "#     Y: onehot encoded\n",
    "#     \"\"\"\n",
    "#     Z = - X @ W\n",
    "#     N = X.shape[0]\n",
    "#     loss = 1/N * (np.trace(X @ W @ Y.T) + np.sum(np.log(np.sum(np.exp(Z), axis=1))))\n",
    "#     return loss\n",
    "\n",
    "# def gradient(X, Y, W, mu):\n",
    "#     \"\"\"\n",
    "#     Y: onehot encoded \n",
    "#     \"\"\"\n",
    "#     Z = - X @ W\n",
    "#     P = softmax(Z, axis=1)\n",
    "#     N = X.shape[0]\n",
    "#     gd = 1/N * (X.T @ (Y - P)) + 2 * mu * W\n",
    "#     return gd\n",
    "\n",
    "# def gradient_descent(X, Y, max_iter=1000, eta=0.1, mu=0.01):\n",
    "#     \"\"\"\n",
    "#     Very basic gradient descent algorithm with fixed eta and mu\n",
    "#     \"\"\"\n",
    "#     Y_onehot = onehot_encoder.fit_transform(Y.reshape(-1,1))\n",
    "#     W = np.zeros((X.shape[1], Y_onehot.shape[1]))\n",
    "#     step = 0\n",
    "#     step_lst = [] \n",
    "#     loss_lst = []\n",
    "#     W_lst = []\n",
    " \n",
    "#     while step < max_iter:\n",
    "#         step += 1\n",
    "#         W -= eta * gradient(X, Y_onehot, W, mu)\n",
    "#         step_lst.append(step)\n",
    "#         W_lst.append(W)\n",
    "#         loss_lst.append(loss(X, Y_onehot, W))\n",
    "\n",
    "#     df = pd.DataFrame({\n",
    "#         'step': step_lst, \n",
    "#         'loss': loss_lst\n",
    "#     })\n",
    "#     return df, W\n",
    "\n",
    "# class Multiclass:\n",
    "#     def fit(self, X, Y):\n",
    "#         self.loss_steps, self.W = gradient_descent(X, Y)\n",
    "\n",
    "#     def loss_plot(self):\n",
    "#         return self.loss_steps.plot(\n",
    "#             x='step', \n",
    "#             y='loss',\n",
    "#             xlabel='step',\n",
    "#             ylabel='loss'\n",
    "#         )\n",
    "\n",
    "#     def predict(self, H):\n",
    "#         Z = - H @ self.W\n",
    "#         P = softmax(Z, axis=1)\n",
    "#         return np.argmax(P, axis=1)\n",
    "    \n",
    "# X = load_iris().data\n",
    "# Y = load_iris().target\n",
    "\n",
    "# # fit model\n",
    "# model = Multiclass()\n",
    "# model.fit(X, Y)\n",
    "\n",
    "# # plot loss\n",
    "# model.loss_plot()\n",
    "\n",
    "# # predict \n",
    "# model.predict(X)\n",
    "\n",
    "# # check the predicted value and the actual value\n",
    "# model.predict(X) == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
